{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this notebook is to present to the user the changes that occured in a delta table.\n",
        "\n",
        "It takes advantange of the delta versioning and dynamically generates the base SQL for the results.\n",
        "\n",
        "For this to work, an \"information_schema\" table needs to be filled with the metadata of each table to process. This assumes there's no information schema to get this information from i.e. no Unity Catalog.\n",
        "\n",
        "Author: Luis Soares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\n",
        "debug = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some information was taken from: https://learn.microsoft.com/en-us/azure/databricks/delta/delta-change-data-feed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "-- Create the \"information_schema\" table\n",
        "-- PK columns MUST be 1st in the list\n",
        "-- \n",
        "DROP TABLE IF EXISTS information_schema;\n",
        "\n",
        "CREATE TABLE information_schema(table_name STRING, column_name STRING, column_pos INT, is_key BOOLEAN, is_ignore BOOLEAN);\n",
        "\n",
        "INSERT INTO information_schema VALUES('students', 'id',    1, true,  false),\n",
        "                                     ('students', 'name',  2, false, true),\n",
        "                                     ('students', 'grade', 3, false, false),\n",
        "                                     ('students', 'year',  4, false, false),\n",
        "                                     ('students', 'age',   5, false, false);\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "-- This is a data table to be the target of fetching changes\n",
        "\n",
        "DROP TABLE IF EXISTS students; -- droped to clean any previous delta changes\n",
        "\n",
        "-- Creation will be change # 0\n",
        "CREATE TABLE students(id INT, name STRING, grade DOUBLE, year INT, age INT)\n",
        "USING DELTA\n",
        "TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
        "\n",
        "-- Insert will be change # 1\n",
        "INSERT INTO students VALUES(4, \"Ted\",     4.7, 2020, 20),\n",
        "                           (5, \"Tiffany\", 5.5, 2021, 21),\n",
        "                           (6, \"Vini\",    6.3, 2022, 22);\n",
        "\n",
        "-- Update will be change # 2\n",
        "UPDATE students SET grade = grade + 1, year = year + 1 WHERE name LIKE \"T%\";\n",
        "\n",
        "SELECT * FROM students;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "DESCRIBE EXTENDED students -- Property \"Provider\" shows it's a delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "DESCRIBE HISTORY default.students -- List delta changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Different ways to get delta changes\r\n",
        "\r\n",
        "# Between 2 delta versions (as ints or longs e.g. changes from version 1 to 2)\r\n",
        "#df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 1).option(\"endingVersion\", 2).table('students')\r\n",
        "\r\n",
        "# Between timestamps (as string formatted timestamps)\r\n",
        "#df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingTimestamp\", '2021-04-21 05:45:46').option(\"endingTimestamp\", '2021-05-21 12:00:00').table('students')\r\n",
        "\r\n",
        "# Providing only the starting version/timestamp\r\n",
        "df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 2).table('students')\r\n",
        "\r\n",
        "# With database/schema names inside the string for table name, with backticks for escaping dots and special characters\r\n",
        "# SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\r\n",
        "\r\n",
        "# With path based tables\r\n",
        "# SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\r\n",
        "\r\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Create a temporary view to the delta changes\r\n",
        "\r\n",
        "df.createOrReplaceTempView('V_CHANGE_students')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "-- This is the query we want to generate dynamically\n",
        "\n",
        "SELECT\n",
        "  s1.id,\n",
        "  CASE WHEN s1.grade <> s2.grade THEN concat(s1.grade, ' -> ', s2.grade) ELSE '' END AS grade,\n",
        "  CASE WHEN s1.year  <> s2.year  THEN concat(s1.year,  ' -> ', s2.year)  ELSE '' END AS year,\n",
        "  CASE WHEN s1.age   <> s2.age   THEN concat(s1.age,   ' -> ', s2.age)   ELSE '' END AS age,\n",
        "  now() as sysdate\n",
        "FROM V_CHANGE_students s1\n",
        "INNER JOIN V_CHANGE_students s2 ON (s2.id = s1.id)\n",
        "WHERE s1._change_type IN ('update_preimage', 'insert') AND s2._change_type = 'update_postimage' AND\n",
        "      (s1.grade <> s2.grade OR s1.year <> s2.year OR s1.age <> s2.age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "# Dynamically generate the SQL queries for all tables in the \"information_schema\" table\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "table_list = spark.sql('SELECT DISTINCT(table_name) AS table_name FROM information_schema').toPandas()\n",
        "if(debug): print(table_list)\n",
        "\n",
        "final = [] # list of final results\n",
        "\n",
        "for indx, tables_row in table_list.iterrows():\n",
        "    table_name = tables_row['table_name']\n",
        "    print('Processing table ' + table_name)\n",
        "\n",
        "    sql = \"SELECT column_name, is_key FROM information_schema WHERE table_name = '\" + table_name + \"' AND is_ignore = false ORDER BY column_pos\"\n",
        "    column_list = spark.sql(sql).toPandas()\n",
        "    if(debug): print(column_list)\n",
        "\n",
        "    full_sql   = 'SELECT ' # the SQL query for each table\n",
        "    key_sql    = ''        # the condition for the self join\n",
        "    pred_sql   = ''        # the comparison between all value columns\n",
        "    id_vars    = []        # list of PK's for the pivot operation\n",
        "    value_vars = []        # list of value columns for the pivot operation\n",
        "\n",
        "    for indx2, column_row in column_list.iterrows():\n",
        "        column_name = column_row['column_name']\n",
        "        is_key      = column_row['is_key']\n",
        "\n",
        "        if(is_key):\n",
        "            full_sql = full_sql + '\\ns1.' + column_name + ',\\n'\n",
        "            if(key_sql != ''): key_sql = key_sql + \" AND \"\n",
        "            key_sql = key_sql + \"s2.\" + column_name + \" = s1.\" + column_name\n",
        "            id_vars = id_vars + [column_name]\n",
        "        else:\n",
        "            full_sql = full_sql + \"CASE WHEN s1.\" + column_name + \" <> s2.\" + column_name + \" THEN \" \\\n",
        "                                \"concat(s1.\" + column_name + \", ' -> ', s2.\" + column_name + \") \" \\\n",
        "                                \"ELSE '' END AS \" + column_name + \",\\n\"\n",
        "            pred_sql = pred_sql + (\"(\" if pred_sql == '' else \" OR \")\n",
        "            pred_sql = pred_sql + \"s1.\" + column_name + \" <> s2.\" + column_name\n",
        "            value_vars = value_vars + [column_name]\n",
        "\n",
        "    full_sql = full_sql + \"now() as sysdate\\n\" + \\\n",
        "                        \"FROM V_CHANGE_\" + table_name + \" s1\\n\" + \\\n",
        "                        \"INNER JOIN V_CHANGE_\" + table_name + \" s2 ON (\" + key_sql + \")\\n\" + \\\n",
        "                        \"WHERE s1._change_type IN ('update_preimage', 'insert') AND s2._change_type = 'update_postimage' AND\\n\" + \\\n",
        "                        pred_sql + \")\"\n",
        "    \n",
        "    if(debug): print('SQL:\\n' + full_sql)\n",
        "\n",
        "    final.append([table_name, full_sql, id_vars, value_vars])\n",
        "\n",
        "    if(debug): print('id_vars:', id_vars)\n",
        "    if(debug): print('value_vars:', value_vars)\n",
        "\n",
        "if(debug): print('final:', final)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "SELECT \n",
        "s1.id,\n",
        "CASE WHEN s1.grade <> s2.grade THEN concat(s1.grade, ' -> ', s2. grade) ELSE '' END AS grade,\n",
        "CASE WHEN s1.year <> s2.year THEN concat(s1.year, ' -> ', s2. year) ELSE '' END AS year,\n",
        "CASE WHEN s1.age <> s2.age THEN concat(s1.age, ' -> ', s2. age) ELSE '' END AS age,\n",
        "now() as sysdate\n",
        "FROM V_CHANGE_students s1\n",
        "INNER JOIN V_CHANGE_students s2 ON (s2.id = s1.id)\n",
        "WHERE s1._change_type IN ('update_preimage', 'insert') AND s2._change_type = 'update_postimage' AND\n",
        "(s1.grade <> s2.grade OR s1.year <> s2.year OR s1.age <> s2.age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "# Get results for one table, melt (unpivot), clean\n",
        "\n",
        "table_name, full_sql, id_vars, value_vars = final[0][0], final[0][1], final[0][2], final[0][3]\n",
        "if(debug): print('table_name:', table_name)\n",
        "if(debug): print('full_sql:',   full_sql)\n",
        "if(debug): print('id_vars:',    id_vars)\n",
        "if(debug): print('value_vars:', value_vars)\n",
        "\n",
        "# execute SQL quesry\n",
        "df = spark.sql(full_sql).toPandas()\n",
        "display(df)\n",
        "\n",
        "# melt the dataset\n",
        "df_melted = df.melt(id_vars, value_vars).sort_values(by=id_vars + ['variable'])\n",
        "display(df_melted)\n",
        "\n",
        "# clean the dataset\n",
        "df_melted_clean = df_melted[df_melted['value'] != '']\n",
        "display(df_melted_clean)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to select the changes between 2 versions (or timestamps), then we may have several changes applied to the same PK.\n",
        "This means we need to extract only the oldest and newest record for each PK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "\n",
        "# Select changes between 2 versions\n",
        "\n",
        "df = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 1).option(\"endingVersion\", 2).table('students').orderBy(\"ID\", \"_commit_version\")\n",
        "\n",
        "df.createOrReplaceTempView('v_all_changes_students')\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "-- Create a temporaty view for a result that holds multiple changes for each PK into a table with 1-2 records per PK (oldest and newest)\n",
        "\n",
        "-- We can recreate the same view as before as it has the same structure\n",
        "-- This view can also be generated dynamically\n",
        "CREATE OR REPLACE TEMP VIEW V_CHANGE_students AS\n",
        "SELECT * FROM v_all_changes_students s1\n",
        "WHERE (\n",
        "  s1._commit_timestamp = (SELECT MAX(s2._commit_timestamp) FROM v_all_changes_students s2 WHERE s2.id = s1.id AND s2._change_type = s2._change_type)\n",
        "  AND s1._change_type IN ('update_postimage', 'insert')\n",
        ") OR (\n",
        "s1._commit_timestamp = (SELECT MIN(s2._commit_timestamp) FROM v_all_changes_students s2 WHERE s2.id = s1.id AND s2._change_type = s2._change_type)\n",
        "AND s1._change_type IN ('update_preimage', 'insert')\n",
        ")\n",
        "ORDER BY id, _commit_timestamp DESC;\n",
        "\n",
        "SELECT * FROM V_CHANGE_students ORDER BY ID, _commit_version\n",
        "\n",
        "-- We can now use this view as an input to the generation of changes, as before"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "sql"
    }
  }
}